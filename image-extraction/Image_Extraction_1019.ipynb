{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('/Users/sibylhe/text-detection-ctpn') # path to ctpn\n",
    "from lib.networks.factory import get_network\n",
    "from lib.fast_rcnn.config import cfg, cfg_from_file\n",
    "from lib.fast_rcnn.test import test_ctpn\n",
    "from lib.utils.timer import Timer\n",
    "from lib.text_connector.detectors import TextDetector\n",
    "from lib.text_connector.text_connect_cfg import Config as TextLineCfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEST_IMAGES_DIR = '/Users/sibylhe/Documents/DR/image_extraction/image/test181019/'\n",
    "image_names = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
    "if '.DS_Store' in image_names:\n",
    "    image_names.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11280+Olympic+Blvd.%2C+Los+Angeles%2C+CA.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_im(im, scale, max_scale=None):\n",
    "    f = float(scale) / min(im.shape[0], im.shape[1])\n",
    "    if max_scale != None and f * max(im.shape[0], im.shape[1]) > max_scale:\n",
    "        f = float(max_scale) / max(im.shape[0], im.shape[1])\n",
    "    return cv2.resize(im, None, None, fx=f, fy=f, interpolation=cv2.INTER_LINEAR), f\n",
    "\n",
    "def format_text_boxes(boxes, img_w, img_h):\n",
    "    fomatted_boxes_list = []\n",
    "    for box in boxes:\n",
    "        min_x = min(box[0], box[2], box[4], box[6])/img_w\n",
    "        min_y = min(box[1], box[3], box[5], box[7])/img_h\n",
    "        max_x = max(box[0], box[2], box[4], box[6])/img_w\n",
    "        max_y = max(box[1], box[3], box[5], box[7])/img_h\n",
    "        score = box[8]\n",
    "        fomatted_box = np.array([min_y, min_x, max_y, max_x, score])\n",
    "        fomatted_boxes_list.append(fomatted_box)\n",
    "    fomatted_boxes = np.array(fomatted_boxes_list)\n",
    "    return fomatted_boxes\n",
    "\n",
    "def filter_box_size(boxes, threshold=0.004):\n",
    "    qualified_boxes_list = []\n",
    "    for box in boxes:\n",
    "        min_x = box[1]\n",
    "        min_y = box[0]\n",
    "        max_x = box[3]\n",
    "        max_y = box[2]\n",
    "        box_size = (max_x-min_x)*(max_y-min_y)\n",
    "        if box_size >= threshold: # 0.0035 is the threshold to filter out google watermarks. Threshold to be defined. \n",
    "            qualified_boxes_list.append(box)\n",
    "    qualified_boxes = np.array(qualified_boxes_list)\n",
    "    return qualified_boxes\n",
    "\n",
    "\n",
    "def ctpn(sess, net, image_name):\n",
    "    timer = Timer()\n",
    "    timer.tic()\n",
    "\n",
    "    img = cv2.imread(image_name)\n",
    "    img, scale = resize_im(img, scale=TextLineCfg.SCALE, max_scale=TextLineCfg.MAX_SCALE)\n",
    "    img_w = img.shape[0]\n",
    "    img_h = img.shape[1]\n",
    "    scores, boxes = test_ctpn(sess, net, img)\n",
    "\n",
    "    textdetector = TextDetector()\n",
    "    boxes = textdetector.detect(boxes, scores[:, np.newaxis], img.shape[:2])\n",
    "    \n",
    "    text_boxes = format_text_boxes(boxes, img_w, img_h)\n",
    "    text_boxes = filter_box_size(text_boxes, threshold=0.004)\n",
    "    # text_boxes = [min_y, min_x, max_y, max_x, score]\n",
    "    \n",
    "    #draw_boxes(img, image_name, text_boxes, scale)\n",
    "    timer.toc()\n",
    "    print(('Detection took {:.3f}s for '\n",
    "           '{:d} object proposals').format(timer.total_time, text_boxes.shape[0]))\n",
    "    return text_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "Tensor(\"conv5_3/conv5_3:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "Tensor(\"rpn_conv/3x3/rpn_conv/3x3:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "Tensor(\"lstm_o/Reshape_2:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "Tensor(\"lstm_o/Reshape_2:0\", shape=(?, ?, ?, 512), dtype=float32)\n",
      "Tensor(\"rpn_cls_score/Reshape_1:0\", shape=(?, ?, ?, 20), dtype=float32)\n",
      "Tensor(\"rpn_cls_prob:0\", shape=(?, ?, ?, ?), dtype=float32)\n",
      "Tensor(\"Reshape_2:0\", shape=(?, ?, ?, 20), dtype=float32)\n",
      "Tensor(\"rpn_bbox_pred/Reshape_1:0\", shape=(?, ?, ?, 40), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 3), dtype=float32)\n",
      "Loading network VGGnet_test...  Restoring from /Users/sibylhe/text-detection-ctpn/checkpoints/VGGnet_fast_rcnn_iter_50000.ckpt... INFO:tensorflow:Restoring parameters from /Users/sibylhe/text-detection-ctpn/checkpoints/VGGnet_fast_rcnn_iter_50000.ckpt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#if os.path.exists(\"data/results/\"):\n",
    "#    shutil.rmtree(\"data/results/\")\n",
    "#os.makedirs(\"data/results/\")\n",
    "\n",
    "cfg_from_file('/Users/sibylhe/text-detection-ctpn/ctpn/text.yml') #path to ctpn/text.yml\n",
    "\n",
    "# init session\n",
    "#tf.get_variable_scope().reuse_variables() #comment out if 1st time run\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "sess = tf.Session(config=config)\n",
    "# load network\n",
    "net = get_network(\"VGGnet_test\")\n",
    "# load model\n",
    "print(('Loading network {:s}... '.format(\"VGGnet_test\")), end=' ')\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "'''\n",
    "/Users/sibylhe/text-detection-ctpn/ctpn/text.yml\n",
    "line 37: checkpoints_path: /Users/sibylhe/text-detection-ctpn/checkpoints/\n",
    "'''\n",
    "\n",
    "try:\n",
    "    ckpt = tf.train.get_checkpoint_state(cfg.TEST.checkpoints_path)\n",
    "    print('Restoring from {}...'.format(ckpt.model_checkpoint_path), end=' ')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print('done')\n",
    "except:\n",
    "    raise 'Check your pretrained {:s}'.format(ckpt.model_checkpoint_path)\n",
    "\n",
    "im = 128 * np.ones((300, 300, 3), dtype=np.uint8)\n",
    "for i in range(2):\n",
    "    _, _ = test_ctpn(sess, net, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LQSEnEsPWKMj"
   },
   "source": [
    "## mask_rcnn_inception_v2_coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n",
    "\n",
    "See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "from distutils.version import StrictVersion\n",
    "import numpy as np\n",
    "import six.moves.urllib as urllib\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(\"/anaconda3/lib/python3.6/site-packages/tensorflow/models/research\") #parent folder of object-detection\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "    raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n",
    "\n",
    "# imports from the object detection module\n",
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "92BHxzcNWKMf"
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in ['num_detections', 'detection_boxes', \n",
    "                        'detection_scores','detection_classes', 'detection_masks']:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "                        \n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-st time (need to download model)\n",
    "# What model to download.\n",
    "MODEL_NAME = 'mask_rcnn_inception_v2_coco_2018_01_28'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "# download model\n",
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "    file_name = os.path.basename(file.name)\n",
    "    if 'frozen_inference_graph.pb' in file_name:\n",
    "        tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd time and after (model has been downloaded)\n",
    "MODEL_NAME = 'mask_rcnn_inception_v2_coco_2018_01_28'\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a (frozen) Tensorflow model into memory.\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "# Load label map\n",
    "NUM_CLASSES = 90\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "# Generate positive class list\n",
    "positive_name = ['stop sign','traffic light','clock','bench', 'potted plant', 'fire hydrant', 'parking meter', 'toilet']\n",
    "positive_id =[]\n",
    "for i in range(len(positive_name)):\n",
    "    name = positive_name[i]\n",
    "    for j in category_index.keys():\n",
    "        if category_index[j]['name'] == name:\n",
    "            positive_id.append(category_index[j]['id'])\n",
    "# positive_id = [13, 10, 85, 15, 64, 11, 14, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 10, 85, 15, 64, 11, 14, 70]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_neg_boxes(output_dict):\n",
    "    num_detections = output_dict['num_detections']\n",
    "    detection_boxes = output_dict['detection_boxes'][:num_detections]\n",
    "    detection_classes = output_dict['detection_classes'][:num_detections]\n",
    "    detection_scores = output_dict['detection_scores'][:num_detections]\n",
    "    \n",
    "    pos_boxes_list = []\n",
    "    neg_boxes_list = []\n",
    "    for i in range(num_detections):\n",
    "        if detection_classes[i] in positive_id:\n",
    "            detection_box = np.append(detection_boxes[i],detection_scores[i])\n",
    "            pos_boxes_list.append(detection_box)\n",
    "        else:\n",
    "            neg_boxes_list.append(detection_boxes[i])\n",
    "    pos_boxes = np.array(pos_boxes_list)\n",
    "    neg_boxes = np.array(neg_boxes_list)\n",
    "    #pos_boxes = [min_y, min_x, max_y, max_x, score]\n",
    "    #neg_boxes = [min_y, min_x, max_y, max_x]\n",
    "    \n",
    "    return pos_boxes, neg_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_neg_text_boxes(text_boxes, neg_boxes, threshold=0.5):\n",
    "    '''\n",
    "    If a text_box overlaps with a neg_box and overlapping area >= 0.5 * text_box size:\n",
    "    eliminate the text_box\n",
    "    '''\n",
    "    # [min_y, min_x, max_y, max_x]\n",
    "    to_delete = []\n",
    "    for i in range(len(text_boxes)):\n",
    "        t = text_boxes[i]\n",
    "        for n in neg_boxes:\n",
    "            '''\n",
    "            notOverlapped: \n",
    "            t.max_x < n.min_x | t.max_y < n.min_y | t.min_x > n.max_x | t.min_y < n.max_y\n",
    "            '''\n",
    "            notOverlapped = (t[3]<n[1])|(t[2]<n[0])|(t[1]>n[3])|(t[0]<n[2])\n",
    "            \n",
    "            if notOverlapped is False:\n",
    "                t_size = abs((t[3]-t[1])*(t[2]-t[0]))\n",
    "                \n",
    "                o_min_x = max(t[1], n[1])\n",
    "                o_min_y = max(t[0], n[0])  \n",
    "                o_max_x = min(t[3], n[3])  \n",
    "                o_max_y = min(t[2], n[2])  \n",
    "                o_size = abs((o_max_x-o_min_x)*(o_max_y-o_min_y))\n",
    "                \n",
    "                if o_size/t_size >= threshold:\n",
    "                    to_delete.append(i)\n",
    "\n",
    "    if len(to_delete) > 0:\n",
    "        text_boxes = np.delete(text_boxes, to_delete, axis=0)\n",
    "    return text_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top10_size(boxes):\n",
    "    new_boxes_list = []\n",
    "    for box in boxes:\n",
    "        box_size = abs((box[2]-box[0])*(box[3]-box[1]))\n",
    "        box = np.append(box, box_size)\n",
    "        new_boxes_list.append(box)\n",
    "    new_boxes = np.array(new_boxes_list)\n",
    "    arg = np.argsort(new_boxes[:,5])\n",
    "    top10_boxes = new_boxes[arg]\n",
    "    top10_boxes = top10_boxes[::-1][:10]\n",
    "    return top10_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top10_score(boxes):\n",
    "    arg = np.argsort(boxes[:,4])\n",
    "    top10_boxes = boxes[arg][::-1][:10]\n",
    "    return top10_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxes in ratio -> absolute coordinates\n",
    "def format_abs_boxes(boxes, h, w):\n",
    "    abs_box_list = []\n",
    "    for box in boxes:\n",
    "        #[min_y, min_x, max_y, max_x, score] -> [min_x*w, min_y*h, max_x*w, max_y*h, score]\n",
    "        abs_box = np.array([box[1]*w, box[0]*h, box[3]*w, box[2]*h, box[4]])\n",
    "        abs_box_list.append(abs_box)\n",
    "    abs_boxes = np.array(abs_box_list)\n",
    "    return abs_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_path, image_name, boxes, outpath):\n",
    "    #with open(outpath + 'res_{}.txt'.format(image_name.split('.')[0]), 'w') as f:\n",
    "    img = cv2.imread(image_path)\n",
    "    for box in boxes:\n",
    "        #[min_x*w, min_y*h, max_x*w, max_y*h, score]\n",
    "        if np.linalg.norm(box[0] - box[1]) < 5 or np.linalg.norm(box[3] - box[0]) < 5:\n",
    "            continue\n",
    "        if box[4] >= 0.9:\n",
    "            color = (0, 255, 0)\n",
    "        elif box[4] >= 0.8:\n",
    "            color = (255, 0, 0)\n",
    "        cv2.line(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[1])), color, 2)\n",
    "        cv2.line(img, (int(box[2]), int(box[1])), (int(box[2]), int(box[3])), color, 2)\n",
    "        cv2.line(img, (int(box[2]), int(box[3])), (int(box[0]), int(box[3])), color, 2)\n",
    "        cv2.line(img, (int(box[0]), int(box[3])), (int(box[0]), int(box[1])), color, 2)\n",
    "        \n",
    "        #line = str(list(box)).strip('[|]')+'\\n'\n",
    "        #f.write(line)\n",
    "\n",
    "    cv2.imwrite(outpath+image_name, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Demo for /Users/sibylhe/Documents/DR/image_extraction/image/test181019/11280+Olympic+Blvd.%2C+Los+Angeles%2C+CA.jpg\n",
      "Detection took 5.404s for 3 object proposals\n",
      "Elaspe:  19.419419765472412 s\n"
     ]
    }
   ],
   "source": [
    "# Size, in inches, of the output images.\n",
    "#IMAGE_SIZE = (12,8)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for image_name in image_names:\n",
    "    image_path = PATH_TO_TEST_IMAGES_DIR+image_name\n",
    "    \n",
    "    # CTPN\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print(('Demo for {:s}'.format(image_path)))\n",
    "    text_boxes = ctpn(sess, net, image_path)\n",
    "    \n",
    "    # Mask RCNN\n",
    "    image = Image.open(image_path)\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    w = image_np.shape[0] # original width\n",
    "    h = image_np.shape[1] # original height\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    \n",
    "    # Eliminate negative boxes and too small positive boxes\n",
    "    pos_boxes, neg_boxes = filter_neg_boxes(output_dict)\n",
    "    pos_boxes = filter_box_size(pos_boxes, threshold=0.002)\n",
    "    \n",
    "    # Eliminate text boxes overlapping with negative boxes\n",
    "    text_boxes = filter_neg_text_boxes(text_boxes, neg_boxes, threshold=0.5)\n",
    "    \n",
    "    output_boxes = np.concatenate((text_boxes, pos_boxes), axis=0)\n",
    "    # Filter top10 boxes by size/score\n",
    "    if len(output_boxes) > 10:\n",
    "        output_boxes = filter_top10_size(output_boxes)   # by size\n",
    "        #output_boxes = filter_top10_score(output_boxes)  # by score\n",
    "    \n",
    "    # Format output_boxes represented in ratio to abs_boxes in absolute coordinates\n",
    "    abs_boxes = format_abs_boxes(output_boxes, h, w)\n",
    "    \n",
    "    # Visualize boxes and write output txt\n",
    "    draw_boxes(image_path, image_name, abs_boxes, outpath='/Users/sibylhe/Documents/DR/image_extraction/image/result181019/')\n",
    "    \n",
    "end = time.time()\n",
    "print('Elaspe: ',end-start,'s')\n",
    "# 19+ sec per image on 8GB RAM CPU "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
